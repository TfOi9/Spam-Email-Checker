import random
from sklearn.metrics import classification_report
import joblib
import os
import re

class SpamDisguiser:
    def __init__(self):
        self.normal_patterns = [
            "Hi team, I wanted to follow up on",
            "Hello, I hope this email finds you well.",
            "Dear colleagues, regarding our recent discussion about",
            "Good morning, I'm writing to update you on"
        ]
        
        self.normal_sentences = [
            "The project deadline has been moved to next Friday.",
            "Please review the attached document and provide feedback.",
            "Our team meeting has been rescheduled for 3 PM tomorrow.",
            "I've updated the shared drive with the latest files."
        ]
        
        self.spam_replacements = {
            'free': 'complimentary', 'win': 'receive', 'prize': 'award',
            'click': 'visit', 'buy': 'acquire', 'discount': 'savings',
            'limited': 'exclusive', 'offer': 'opportunity', '!!!': '.',
            'URGENT': 'Important', 'guarantee': 'assurance', '$': 'USD'
        }
    
    def disguise_method1(self, text):
        """æ–¹æ³•1ï¼šæ·»åŠ æ­£å¸¸é‚®ä»¶å¼€å¤´"""
        opening = random.choice(self.normal_patterns)
        return f"{opening} {text}"
    
    def disguise_method2(self, text):
        """æ–¹æ³•2ï¼šæ›¿æ¢åƒåœ¾é‚®ä»¶è¯æ±‡"""
        for spam_word, normal_word in self.spam_replacements.items():
            text = text.replace(spam_word, normal_word)
        return text
    
    def disguise_method3(self, text):
        """æ–¹æ³•3ï¼šæ··åˆæ­£å¸¸å†…å®¹"""
        normal = random.choice(self.normal_sentences)
        transitions = ["By the way,", "On a different note,", "Additionally,"]
        transition = random.choice(transitions)
        
        if random.random() > 0.5:
            return f"{normal} {transition} {text}"
        else:
            return f"{text} {transition} {normal}"
    
    def disguise_method4(self, text):
        """æ–¹æ³•4ï¼šç»„åˆå¤šç§æ–¹æ³•"""
        text = self.disguise_method2(text)  # å…ˆæ›¿æ¢è¯æ±‡
        text = self.disguise_method1(text)  # å†æ·»åŠ å¼€å¤´
        return text
    
    def generate_disguised_samples(self, spam_texts, num_samples_per_method=5):
        """ä¸ºæ¯ä¸ªåƒåœ¾é‚®ä»¶ç”Ÿæˆä¼ªè£…ç‰ˆæœ¬"""
        disguised_samples = []
        
        for spam_text in spam_texts:
            # åŸå§‹æ ·æœ¬
            disguised_samples.append(("åŸå§‹", spam_text, 1))
            
            # æ–¹æ³•1
            for _ in range(num_samples_per_method):
                disguised = self.disguise_method1(spam_text)
                disguised_samples.append(("æ–¹æ³•1", disguised, 1))
            
            # æ–¹æ³•2
            for _ in range(num_samples_per_method):
                disguised = self.disguise_method2(spam_text)
                disguised_samples.append(("æ–¹æ³•2", disguised, 1))
            
            # æ–¹æ³•3
            for _ in range(num_samples_per_method):
                disguised = self.disguise_method3(spam_text)
                disguised_samples.append(("æ–¹æ³•3", disguised, 1))
            
            # æ–¹æ³•4
            for _ in range(num_samples_per_method):
                disguised = self.disguise_method4(spam_text)
                disguised_samples.append(("æ–¹æ³•4", disguised, 1))
        
        return disguised_samples

def test_model_robustness(model, vectorizer, test_spam_emails):
    """æµ‹è¯•æ¨¡å‹å¯¹ä¼ªè£…åƒåœ¾é‚®ä»¶çš„è¯†åˆ«èƒ½åŠ›"""
    disguiser = SpamDisguiser()
    disguised_samples = disguiser.generate_disguised_samples(test_spam_emails)
    
    results = []
    
    for method, email, true_label in disguised_samples:
        # é¢„å¤„ç†å’Œé¢„æµ‹
        processed = complete_preprocess(email)
        email_vector = vectorizer.transform([processed])
        
        if hasattr(model, 'predict_proba'):
            email_dense = email_vector.toarray()
            prediction = model.predict(email_dense)[0]
            probability = model.predict_proba(email_dense)[0]
            spam_prob = probability[1]
        else:
            if method == "HistGradientBoosting":
                email_dense = email_vector.toarray()
                prediction = model.predict(email_dense)[0]
            else:
                prediction = model.predict(email_vector)[0]
            spam_prob = 0.5  # å¦‚æœæ²¡æœ‰æ¦‚ç‡ï¼Œè®¾ä¸ºä¸­æ€§
        
        is_correct = (prediction == true_label)
        results.append((method, email, true_label, prediction, spam_prob, is_correct))
    
    return results

def analyze_robustness_results(results):
    """åˆ†æå¯¹æŠ—æ€§æµ‹è¯•ç»“æœ"""
    from collections import defaultdict
    import pandas as pd
    
    method_stats = defaultdict(lambda: {'total': 0, 'correct': 0, 'spam_probs': []})
    
    for method, email, true_label, prediction, spam_prob, is_correct in results:
        method_stats[method]['total'] += 1
        method_stats[method]['spam_probs'].append(spam_prob)
        if is_correct:
            method_stats[method]['correct'] += 1
    
    print("=== æ¨¡å‹é²æ£’æ€§åˆ†æ ===")
    for method, stats in method_stats.items():
        accuracy = stats['correct'] / stats['total']
        avg_spam_prob = sum(stats['spam_probs']) / len(stats['spam_probs'])
        print(f"{method}:")
        print(f"  å‡†ç¡®ç‡: {accuracy:.2%}")
        print(f"  å¹³å‡åƒåœ¾é‚®ä»¶æ¦‚ç‡: {avg_spam_prob:.2f}")
        print(f"  æ ·æœ¬æ•°é‡: {stats['total']}")
        print()
    
    return method_stats
def load_emails(folder_path):
    """
    ä»æ–‡ä»¶å¤¹åŠ è½½æ‰€æœ‰é‚®ä»¶æ–‡ä»¶
    folder_path: æ–‡ä»¶å¤¹è·¯å¾„ï¼Œå¦‚ 'data/spam'
    label: æ ‡ç­¾ï¼Œ0è¡¨ç¤ºæ­£å¸¸é‚®ä»¶ï¼Œ1è¡¨ç¤ºåƒåœ¾é‚®ä»¶
    """
    emails = []
    cnt = 0

    # éå†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)
        
        # ç¡®ä¿æ˜¯æ–‡ä»¶è€Œä¸æ˜¯æ–‡ä»¶å¤¹
        if os.path.isfile(file_path):
            cnt += 1
            try:
                # è¯»å–æ–‡ä»¶å†…å®¹ï¼Œæ³¨æ„ç¼–ç é—®é¢˜
                with open(file_path, 'r', encoding='latin-1') as file:
                    content = file.read()
                    emails.append(content)
            except Exception as e:
                print(f"è¯»å–æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
    
    print(f"cnt={cnt}\n")

    return emails
def extract_email_body(raw_email):
    """
    ä»åŸå§‹é‚®ä»¶å†…å®¹ä¸­æå–æ­£æ–‡
    åŸç†ï¼šé‚®ä»¶å¤´ç»“æŸåé€šå¸¸æœ‰ä¸€ä¸ªç©ºè¡Œï¼Œç„¶åæ˜¯æ­£æ–‡
    """
    lines = raw_email.split('\n')
    body_lines = []
    found_empty_line = False
    
    for line in lines:
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªç©ºè¡Œï¼ˆæˆ–åªåŒ…å«ç©ºæ ¼çš„ç©ºè¡Œï¼‰
        if not line.strip():
            found_empty_line = True
            continue
        
        # ç©ºè¡Œä¹‹åçš„å†…å®¹å°±æ˜¯æ­£æ–‡
        if found_empty_line:
            body_lines.append(line)
    
    # å¦‚æœæ²¡æ‰¾åˆ°ç©ºè¡Œï¼Œè¿”å›æ•´ä¸ªå†…å®¹ï¼ˆå¯èƒ½æ ¼å¼å¼‚å¸¸ï¼‰
    if not body_lines:
        return raw_email
    
    return '\n'.join(body_lines)
def complete_preprocess(raw_email):
    """
    å®Œæ•´çš„é‚®ä»¶é¢„å¤„ç†æµç¨‹ï¼š
    1. æå–æ­£æ–‡
    2. æ¸…ç†æ–‡æœ¬
    3. è½¬æ¢ä¸ºå°å†™
    """
    # 1. æå–æ­£æ–‡
    body = extract_email_body(raw_email)
    
    # 2. æ¸…ç†HTMLæ ‡ç­¾
    body = re.sub(r'<.*?>', '', body)
    
    # 3. ç§»é™¤URLs
    body = re.sub(r'http\S+', '', body)
    
    # 4. ç§»é™¤é‚®ç®±åœ°å€
    body = re.sub(r'\S+@\S+', '', body)
    
    # 5. åªä¿ç•™å­—æ¯å’Œç©ºæ ¼ï¼Œç§»é™¤æ•°å­—å’Œç‰¹æ®Šå­—ç¬¦
    body = re.sub(r'[^a-zA-Z\s]', ' ', body)
    
    # 6. è½¬æ¢ä¸ºå°å†™
    body = body.lower()
    
    # 7. ç§»é™¤å¤šä½™ç©ºæ ¼
    body = ' '.join(body.split())
    
    return body
import re

def enhanced_cleaner(text):
    """
    å¢å¼ºçš„æ–‡æœ¬æ¸…ç†å‡½æ•°ï¼Œç§»é™¤å„ç§æŠ€æœ¯æ€§å™ªéŸ³
    """
    if not text:
        return ""
    
    # 1. ç§»é™¤HTMLæ ‡ç­¾å’Œå®ä½“
    text = re.sub(r'<.*?>', '', text)  # ç§»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'&[a-z]+;', '', text)  # ç§»é™¤HTMLå®ä½“å¦‚ &nbsp;
    
    # 2. ç§»é™¤å„ç§URLå’ŒåŸŸå
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
    text = re.sub(r'www\.[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', '', text)  # wwwåŸŸå
    text = re.sub(r'[a-zA-Z0-9.-]+\.(com|org|net|edu|gov|io|co|uk|de|fr|jp|cn)[a-zA-Z0-9./?&=-]*', '', text)  # å„ç§åŸŸå
    
    # 3. ç§»é™¤æ–‡ä»¶è·¯å¾„å’Œæ–‡ä»¶å
    text = re.sub(r'/[a-zA-Z0-9_\-./]+', '', text)  # Unixè·¯å¾„
    text = re.sub(r'[a-zA-Z]:\\[a-zA-Z0-9_\-.\s\\]+', '', text)  # Windowsè·¯å¾„
    text = re.sub(r'[a-zA-Z0-9_\-]+\.[a-zA-Z]{2,4}(?:\s|$)', '', text)  # æ–‡ä»¶å
    
    # 4. ç§»é™¤ç¼–ç å’Œç‰¹æ®Šåºåˆ—
    text = re.sub(r'=[0-9a-fA-F]{2}', '', text)  # URLç¼–ç å¦‚ =3D
    text = re.sub(r'[a-fA-F0-9]{8,}', '', text)  # é•¿åå…­è¿›åˆ¶åºåˆ—
    text = re.sub(r'[0-9a-fA-F]{2}(?::[0-9a-fA-F]{2})+', '', text)  # MACåœ°å€ç­‰
    
    # 5. ç§»é™¤æŠ€æœ¯æ€§å¤´éƒ¨ä¿¡æ¯
    text = re.sub(r'[A-Z][a-zA-Z-]*:\s*[^\n]+', '', text)  # ç±»ä¼¼ Headers: value
    text = re.sub(r'\[[A-Z_]+\]', '', text)  # æ–¹æ‹¬å·å†…çš„æŠ€æœ¯æ ‡ç­¾
    
    # 6. æ¸…ç†æ ‡ç‚¹ç¬¦å·å’Œå¤šä½™ç©ºæ ¼
    text = re.sub(r'[^\w\s]', ' ', text)  # ç§»é™¤éå­—æ¯æ•°å­—å­—ç¬¦ï¼Œä¿ç•™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text)  # åˆå¹¶å¤šä¸ªç©ºæ ¼
    text = text.strip()
    
    return text

def comprehensive_preprocess(raw_email):
    """
    ç»¼åˆé¢„å¤„ç†ï¼šå…ˆæå–æ­£æ–‡ï¼Œå†æ·±åº¦æ¸…ç†
    """
    # 1. æå–é‚®ä»¶æ­£æ–‡
    body = extract_email_body(raw_email)
    
    # 2. åº”ç”¨å¢å¼ºæ¸…ç†
    body = enhanced_cleaner(body)
    
    # 3. è½¬æ¢ä¸ºå°å†™
    body = body.lower()
    
    # 4. æœ€ç»ˆæ¸…ç†
    body = ' '.join(body.split())  # ç§»é™¤å¤šä½™ç©ºæ ¼
    
    return body
# ä½¿ç”¨ç¤ºä¾‹
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer

class AdvancedSpamDisguiser:
    def __init__(self, model, vectorizer):
        self.model = model
        self.vectorizer = vectorizer
        self.feature_names = vectorizer.get_feature_names_out()
        
        # è·å–ç‰¹å¾é‡è¦æ€§
        if hasattr(model, 'coef_'):
            self.feature_importance = model.coef_[0]
        elif hasattr(model, 'feature_importances_'):
            self.feature_importance = model.feature_importances_
        else:
            self.feature_importance = None
    
    def get_top_spam_features(self, top_n=20):
        """è·å–æœ€é‡è¦çš„åƒåœ¾é‚®ä»¶ç‰¹å¾è¯"""
        if self.feature_importance is None:
            return []
        
        # è·å–å¯¹åƒåœ¾é‚®ä»¶åˆ†ç±»è´¡çŒ®æœ€å¤§çš„ç‰¹å¾
        spam_indices = np.argsort(self.feature_importance)[-top_n:]
        spam_features = [(self.feature_names[i], self.feature_importance[i]) 
                        for i in spam_indices]
        return spam_features
    
    def get_top_ham_features(self, top_n=20):
        """è·å–æœ€é‡è¦çš„æ­£å¸¸é‚®ä»¶ç‰¹å¾è¯"""
        if self.feature_importance is None:
            return []
        
        # è·å–å¯¹æ­£å¸¸é‚®ä»¶åˆ†ç±»è´¡çŒ®æœ€å¤§çš„ç‰¹å¾
        ham_indices = np.argsort(self.feature_importance)[:top_n]
        ham_features = [(self.feature_names[i], self.feature_importance[i]) 
                       for i in ham_indices]
        return ham_features
    
    def strategic_word_replacement(self, text, replacement_ratio=0.3):
        """åŸºäºç‰¹å¾é‡è¦æ€§çš„æˆ˜ç•¥è¯æ±‡æ›¿æ¢"""
        if self.feature_importance is None:
            return text
        
        # è·å–é‡è¦ç‰¹å¾
        top_spam_features = [feat[0] for feat in self.get_top_spam_features(30)]
        top_ham_features = [feat[0] for feat in self.get_top_ham_features(30)]
        
        words = text.lower().split()
        replaced_count = 0
        target_replacements = int(len(words) * replacement_ratio)
        
        for i, word in enumerate(words):
            # å¦‚æœé‡åˆ°åƒåœ¾é‚®ä»¶ç‰¹å¾è¯ï¼Œç”¨æ­£å¸¸é‚®ä»¶ç‰¹å¾è¯æ›¿æ¢
            if word in top_spam_features and replaced_count < target_replacements:
                replacement = np.random.choice(top_ham_features)
                words[i] = replacement
                replaced_count += 1
        
        return ' '.join(words)
class SemanticPreservingRewriter:
    def __init__(self):
        self.synonym_dict = {
            'free': ['complimentary', 'gratis', 'at no cost', 'without charge'],
            'win': ['receive', 'obtain', 'acquire', 'be awarded'],
            'prize': ['award', 'reward', 'gift', 'bonus'],
            'click': ['visit', 'go to', 'navigate to', 'access'],
            'buy': ['purchase', 'acquire', 'invest in', 'obtain'],
            'discount': ['reduction', 'savings', 'deduction', 'markdown'],
            'limited': ['exclusive', 'restricted', 'scarce', 'finite'],
            'offer': ['opportunity', 'proposal', 'arrangement', 'deal'],
            'cash': ['money', 'funds', 'currency', 'payment'],
            'urgent': ['important', 'time-sensitive', 'critical', 'pressing'],
            'guarantee': ['assurance', 'promise', 'warranty', 'pledge'],
            'now': ['immediately', 'promptly', 'without delay', 'right away']
        }
        
        self.normal_email_phrases = [
            "I hope this message finds you well.",
            "I wanted to follow up on our previous conversation.",
            "Please let me know if you have any questions.",
            "Looking forward to your feedback.",
            "Thank you for your time and consideration.",
            "I appreciate your attention to this matter.",
            "Best regards,",
            "Sincerely,",
            "Warm regards,",
            "With appreciation,"
        ]
    
    def advanced_synonym_replacement(self, text, replacement_rate=0.6):
        """é«˜çº§åŒä¹‰è¯æ›¿æ¢ï¼Œä¿æŒè¯­ä¹‰"""
        words = text.split()
        replaced_indices = []
        
        for i, word in enumerate(words):
            word_lower = word.lower().strip('.,!?;:')
            if word_lower in self.synonym_dict and random.random() < replacement_rate:
                synonyms = self.synonym_dict[word_lower]
                # é€‰æ‹©ä¸åŸè¯é•¿åº¦ç›¸è¿‘çš„åŒä¹‰è¯ï¼Œä¿æŒæ–‡æœ¬æµç•…æ€§
                suitable_synonyms = [s for s in synonyms if abs(len(s) - len(word)) <= 2]
                if suitable_synonyms:
                    replacement = random.choice(suitable_synonyms)
                    # ä¿æŒåŸè¯çš„å¤§å°å†™
                    if word[0].isupper():
                        replacement = replacement.capitalize()
                    words[i] = replacement
                    replaced_indices.append(i)
        
        return ' '.join(words)
    
    def context_aware_restructuring(self, text):
        """ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ–‡æœ¬é‡æ„"""
        sentences = text.split('. ')
        if len(sentences) <= 1:
            return text
        
        # åœ¨é€‚å½“ä½ç½®æ’å…¥æ­£å¸¸é‚®ä»¶çŸ­è¯­
        insert_position = random.randint(1, len(sentences) - 1)
        normal_phrase = random.choice(self.normal_email_phrases)
        
        sentences.insert(insert_position, normal_phrase)
        
        # é‡æ–°æ’åˆ—éƒ¨åˆ†å¥å­ï¼ˆä¿æŒé€»è¾‘ï¼‰
        if len(sentences) > 3:
            # åªé‡æ’ä¸­é—´éƒ¨åˆ†ï¼Œä¿æŒå¼€å¤´å’Œç»“å°¾
            middle_start = 1
            middle_end = len(sentences) - 2
            if middle_end > middle_start:
                middle_sentences = sentences[middle_start:middle_end]
                random.shuffle(middle_sentences)
                sentences[middle_start:middle_end] = middle_sentences
        
        return '. '.join(sentences)
    
    def generate_plausible_context(self, spam_core):
        """ä¸ºåƒåœ¾é‚®ä»¶æ ¸å¿ƒå†…å®¹ç”Ÿæˆåˆç†ä¸Šä¸‹æ–‡"""
        contexts = [
            f"I came across this information and thought it might be of interest: {spam_core}",
            f"In my research, I found this opportunity: {spam_core}",
            f"This was shared with me recently and I wanted to pass it along: {spam_core}",
            f"I received this update that might be relevant: {spam_core}",
            f"Here's something that caught my attention: {spam_core}"
        ]
        
        return random.choice(contexts)
class AdvancedAdversarialAttacker:
    def __init__(self, model, vectorizer):
        self.model = model
        self.vectorizer = vectorizer
        self.disguiser = AdvancedSpamDisguiser(model, vectorizer)
        self.rewriter = SemanticPreservingRewriter()
    
    def method1_feature_manipulation(self, text):
        """æ–¹æ³•1ï¼šç‰¹å¾æ“çºµæ”»å‡»"""
        return self.disguiser.strategic_word_replacement(text)
    
    def method2_semantic_rewriting(self, text):
        """æ–¹æ³•2ï¼šè¯­ä¹‰é‡å†™æ”»å‡»"""
        text = self.rewriter.advanced_synonym_replacement(text)
        text = self.rewriter.context_aware_restructuring(text)
        return text
    
    def method3_context_injection(self, text):
        """æ–¹æ³•3ï¼šä¸Šä¸‹æ–‡æ³¨å…¥æ”»å‡»"""
        # æå–æ ¸å¿ƒåƒåœ¾å†…å®¹
        spam_keywords = ['free', 'win', 'prize', 'click', 'buy', 'discount']
        has_spam_content = any(keyword in text.lower() for keyword in spam_keywords)
        
        if has_spam_content:
            return self.rewriter.generate_plausible_context(text)
        return text
    
    def method4_hybrid_attack(self, text, iterations=3):
        """æ–¹æ³•4ï¼šæ··åˆæ”»å‡»ï¼ˆæœ€å¼ºï¼‰"""
        current_text = text
        
        for i in range(iterations):
            # éšæœºé€‰æ‹©å’Œåº”ç”¨æ”»å‡»æ–¹æ³•
            methods = [
                self.method1_feature_manipulation,
                self.method2_semantic_rewriting,
                self.method3_context_injection
            ]
            
            method = random.choice(methods)
            current_text = method(current_text)
            
            # æµ‹è¯•å½“å‰æ–‡æœ¬æ˜¯å¦èƒ½å¤Ÿæ¬ºéª—æ¨¡å‹
            processed = complete_preprocess(current_text)
            vector = self.vectorizer.transform([processed])
            
            if hasattr(self.model, 'predict'):
                dense = vector.toarray()
                prediction = self.model.predict(dense)[0]
                probability = self.model.predict_proba(dense)[0]
                
                # å¦‚æœå·²ç»è¢«åˆ†ç±»ä¸ºæ­£å¸¸é‚®ä»¶ï¼Œæå‰åœæ­¢
                if prediction == 0 and probability[0] > 0.7:
                    print(f"åœ¨ç¬¬ {i+1} æ¬¡è¿­ä»£åæˆåŠŸæ¬ºéª—æ¨¡å‹")
                    break
        
        return current_text
    
    def test_attack_effectiveness(self, original_spam_texts, num_tests=100):
        """æµ‹è¯•æ”»å‡»æ•ˆæœ"""
        results = []
        
        for original_text in original_spam_texts[:num_tests]:
            print(f"\nåŸå§‹åƒåœ¾é‚®ä»¶: {original_text}")
            
            # æµ‹è¯•åŸå§‹æ–‡æœ¬
            original_processed = complete_preprocess(original_text)
            original_vector = self.vectorizer.transform([original_processed])
            original_dense = original_vector.toarray()
            original_pred = self.model.predict(original_dense)[0]
            original_prob = self.model.predict_proba(original_dense)[0]
            
            # åº”ç”¨æ··åˆæ”»å‡»
            attacked_text = self.method4_hybrid_attack(original_text)
            
            # æµ‹è¯•æ”»å‡»åæ–‡æœ¬
            attacked_processed = complete_preprocess(attacked_text)
            attacked_vector = self.vectorizer.transform([attacked_processed])
            attacked_dense = attacked_vector.toarray()
            attacked_pred = self.model.predict(attacked_dense)[0]
            attacked_prob = self.model.predict_proba(attacked_dense)[0]
            
            results.append({
                'original_text': original_text,
                'original_pred': original_pred,
                'original_prob': original_prob,
                'attacked_text': attacked_text,
                'attacked_pred': attacked_pred,
                'attacked_prob': attacked_prob,
                'success': (original_pred == 1 and attacked_pred == 0)
            })
            
            print(f"æ”»å‡»å: {attacked_text}")
            print(f"åŸå§‹é¢„æµ‹: {'åƒåœ¾é‚®ä»¶' if original_pred == 1 else 'æ­£å¸¸é‚®ä»¶'} (æ¦‚ç‡: {original_prob[1]:.3f})")
            print(f"æ”»å‡»åé¢„æµ‹: {'åƒåœ¾é‚®ä»¶' if attacked_pred == 1 else 'æ­£å¸¸é‚®ä»¶'} (æ¦‚ç‡: {attacked_prob[1]:.3f})")
            print(f"æ”»å‡»æˆåŠŸ: {'æ˜¯' if results[-1]['success'] else 'å¦'}")
        
        # ç»Ÿè®¡æˆåŠŸç‡
        success_rate = sum(1 for r in results if r['success']) / len(results)
        print(f"\n=== æ€»ä½“æ”»å‡»æˆåŠŸç‡: {success_rate:.2%} ===")
        
        return results
def create_adversarial_examples_by_transfer(original_texts, target_model, reference_ham_emails):
    """
    é€šè¿‡å‚è€ƒæ­£å¸¸é‚®ä»¶é£æ ¼åˆ›å»ºå¯¹æŠ—æ ·æœ¬
    """
    adversarial_examples = []
    
    # åˆ†ææ­£å¸¸é‚®ä»¶çš„è¯­è¨€æ¨¡å¼
    ham_word_freq = {}
    for email in reference_ham_emails:
        processed = complete_preprocess(email)
        words = processed.split()
        for word in words:
            ham_word_freq[word] = ham_word_freq.get(word, 0) + 1
    
    # è·å–æœ€å¸¸è§çš„æ­£å¸¸é‚®ä»¶è¯æ±‡
    common_ham_words = sorted(ham_word_freq.items(), key=lambda x: x[1], reverse=True)[:50]
    common_ham_words = [word for word, freq in common_ham_words]
    
    for original_text in original_texts:
        words = original_text.split()
        
        # åœ¨åƒåœ¾é‚®ä»¶ä¸­æ’å…¥æ­£å¸¸é‚®ä»¶å¸¸ç”¨è¯
        insert_positions = random.sample(range(len(words)), min(3, len(words)//2))
        for pos in insert_positions:
            if pos < len(words):
                normal_word = random.choice(common_ham_words)
                words.insert(pos, normal_word)
        
        # æ·»åŠ æ­£å¸¸é‚®ä»¶é£æ ¼çš„ç»“å°¾
        normal_endings = [
            "Please let me know if you have any questions.",
            "I look forward to hearing from you.",
            "Thank you for your consideration.",
            "Best regards,",
            "Sincerely,"
        ]
        
        modified_text = ' '.join(words) + " " + random.choice(normal_endings)
        adversarial_examples.append(modified_text)
    
    return adversarial_examples
import pandas as pd
import os
from datetime import datetime

def save_adversarial_results(results, filename=None):
    """å°†å¯¹æŠ—æ ·æœ¬ç»“æœä¿å­˜ä¸ºCSVæ–‡ä»¶"""
    
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f'adversarial_samples_{timestamp}.csv'
    
    # å‡†å¤‡æ•°æ®
    data = []
    for result in results:
        data.append({
            'original_text': result['original_text'],
            'adversarial_text': result['attacked_text'],
            'original_prediction': 'åƒåœ¾é‚®ä»¶' if result['original_pred'] == 1 else 'æ­£å¸¸é‚®ä»¶',
            'adversarial_prediction': 'åƒåœ¾é‚®ä»¶' if result['attacked_pred'] == 1 else 'æ­£å¸¸é‚®ä»¶',
            'original_spam_prob': f"{result['original_prob'][1]:.3f}",
            'adversarial_spam_prob': f"{result['attacked_prob'][1]:.3f}",
            'attack_success': 'æ˜¯' if result['success'] else 'å¦',
            'confidence_change': f"{result['attacked_prob'][1] - result['original_prob'][1]:+.3f}"
        })
    
    # åˆ›å»ºDataFrameå¹¶ä¿å­˜
    df = pd.DataFrame(data)
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    
    print(f"âœ… å¯¹æŠ—æ ·æœ¬å·²ä¿å­˜åˆ°: {filename}")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»æ ·æœ¬æ•°: {len(df)}")
    print(f"  æ”»å‡»æˆåŠŸç‡: {df['attack_success'].value_counts().get('æ˜¯', 0) / len(df):.2%}")
    
    return df
def save_as_readable_text(results, filename=None):
    """ä¿å­˜ä¸ºæ˜“è¯»çš„æ–‡æœ¬æ ¼å¼"""
    
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f'adversarial_samples_{timestamp}.txt'
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write("=== åƒåœ¾é‚®ä»¶å¯¹æŠ—æ ·æœ¬æµ‹è¯•æŠ¥å‘Š ===\n\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ€»æ ·æœ¬æ•°: {len(results)}\n")
        f.write(f"æ”»å‡»æˆåŠŸç‡: {sum(1 for r in results if r['success']) / len(results):.2%}\n\n")
        
        f.write("=" * 80 + "\n")
        
        for i, result in enumerate(results, 1):
            f.write(f"æ ·æœ¬ {i}:\n")
            f.write(f"æ”»å‡»æˆåŠŸ: {'âœ… æ˜¯' if result['success'] else 'âŒ å¦'}\n")
            f.write(f"åŸå§‹åƒåœ¾é‚®ä»¶æ¦‚ç‡: {result['original_prob'][1]:.3f}\n")
            f.write(f"å¯¹æŠ—æ ·æœ¬åƒåœ¾é‚®ä»¶æ¦‚ç‡: {result['attacked_prob'][1]:.3f}\n")
            f.write(f"æ¦‚ç‡å˜åŒ–: {result['attacked_prob'][1] - result['original_prob'][1]:+.3f}\n\n")
            
            f.write("åŸå§‹æ–‡æœ¬:\n")
            f.write(f"{result['original_text']}\n\n")
            
            f.write("å¯¹æŠ—æ–‡æœ¬:\n")
            f.write(f"{result['attacked_text']}\n\n")
            
            f.write("-" * 80 + "\n\n")
    
    print(f"âœ… æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")

import shutil

def organize_adversarial_samples(results, base_dir='adversarial_samples'):
    """æŒ‰æ”»å‡»æ•ˆæœåˆ†ç±»ç»„ç»‡æ ·æœ¬"""
    
    # åˆ›å»ºä¸»ç›®å½•
    if os.path.exists(base_dir):
        shutil.rmtree(base_dir)
    os.makedirs(base_dir)
    
    # åˆ›å»ºå­ç›®å½•
    categories = {
        'high_success': 'é«˜æˆåŠŸç‡ï¼ˆæ¦‚ç‡é™ä½>0.5ï¼‰',
        'medium_success': 'ä¸­ç­‰æˆåŠŸç‡ï¼ˆæ¦‚ç‡é™ä½0.2-0.5ï¼‰', 
        'low_success': 'ä½æˆåŠŸç‡ï¼ˆæ¦‚ç‡é™ä½<0.2ï¼‰',
        'failed': 'æ”»å‡»å¤±è´¥'
    }
    
    for category in categories:
        os.makedirs(os.path.join(base_dir, category))
    
    # åˆ†ç±»ä¿å­˜
    category_counts = {category: 0 for category in categories}
    
    for i, result in enumerate(results):
        prob_change = result['attacked_prob'][1] - result['original_prob'][1]
        
        if prob_change <= -0.5:
            category = 'high_success'
        elif prob_change <= -0.2:
            category = 'medium_success'
        elif prob_change < 0:
            category = 'low_success'
        else:
            category = 'failed'
        
        # ä¿å­˜æ ·æœ¬
        filename = f"sample_{i+1}.txt"
        filepath = os.path.join(base_dir, category, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"åŸå§‹åƒåœ¾é‚®ä»¶æ¦‚ç‡: {result['original_prob'][1]:.3f}\n")
            f.write(f"å¯¹æŠ—æ ·æœ¬åƒåœ¾é‚®ä»¶æ¦‚ç‡: {result['attacked_prob'][1]:.3f}\n")
            f.write(f"æ¦‚ç‡å˜åŒ–: {prob_change:+.3f}\n")
            f.write(f"æ”»å‡»æˆåŠŸ: {result['success']}\n\n")
            
            f.write("åŸå§‹æ–‡æœ¬:\n")
            f.write(result['original_text'] + "\n\n")
            
            f.write("å¯¹æŠ—æ–‡æœ¬:\n")
            f.write(result['attacked_text'] + "\n")
        
        category_counts[category] += 1
    
    # åˆ›å»ºç´¢å¼•æ–‡ä»¶
    with open(os.path.join(base_dir, 'README.txt'), 'w', encoding='utf-8') as f:
        f.write("å¯¹æŠ—æ ·æœ¬åˆ†ç±»è¯´æ˜:\n\n")
        for category, description in categories.items():
            f.write(f"{category}: {description} ({category_counts[category]}ä¸ªæ ·æœ¬)\n")
    
    print(f"âœ… æ ·æœ¬å·²åˆ†ç±»ä¿å­˜åˆ°: {base_dir}/")
    print("ğŸ“ æ–‡ä»¶å¤¹ç»“æ„:")
    for category, count in category_counts.items():
        print(f"  {categories[category]}: {count}ä¸ªæ ·æœ¬")
def save_for_retraining(results, filename='adversarial_training_data.csv'):
    """ä¿å­˜ç”¨äºå¯¹æŠ—è®­ç»ƒçš„æ•¸æ“š"""
    
    training_data = []
    
    for result in results:
        # åŸå§‹æ ·æœ¬ï¼ˆæ ‡ç­¾ä¿æŒä¸ºåƒåœ¾é‚®ä»¶ï¼‰
        training_data.append({
            'text': result['original_text'],
            'label': 1,  # åƒåœ¾é‚®ä»¶
            'type': 'original'
        })
        
        # å¯¹æŠ—æ ·æœ¬ï¼ˆå¦‚æœæ”»å‡»æˆåŠŸï¼Œæ ‡ç­¾ä»ä¸ºåƒåœ¾é‚®ä»¶ï¼›å¦‚æœå¤±è´¥ï¼Œä¿æŒåŸæ ‡ç­¾ï¼‰
        if result['success']:
            # æ”»å‡»æˆåŠŸçš„æ ·æœ¬ï¼Œæ¨¡å‹é”™è¯¯åˆ†ç±»äº†ï¼Œä½†åœ¨è®­ç»ƒä¸­æˆ‘ä»¬åº”è¯¥çº æ­£
            training_data.append({
                'text': result['attacked_text'],
                'label': 1,  # ä»ç„¶æ˜¯åƒåœ¾é‚®ä»¶ï¼
                'type': 'adversarial_success'
            })
        else:
            # æ”»å‡»å¤±è´¥çš„æ ·æœ¬ï¼Œæ¨¡å‹æ­£ç¡®åˆ†ç±»
            training_data.append({
                'text': result['attacked_text'], 
                'label': result['attacked_pred'],
                'type': 'adversarial_failed'
            })
    
    df = pd.DataFrame(training_data)
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    
    print(f"âœ… è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {filename}")
    print(f"ğŸ“Š è®­ç»ƒæ•°æ®ç»Ÿè®¡:")
    print(f"  åŸå§‹æ ·æœ¬: {len(df[df['type'] == 'original'])}")
    print(f"  æˆåŠŸå¯¹æŠ—æ ·æœ¬: {len(df[df['type'] == 'adversarial_success'])}")
    print(f"  å¤±è´¥å¯¹æŠ—æ ·æœ¬: {len(df[df['type'] == 'adversarial_failed'])}")
    
    return df
def comprehensive_save(results, base_dir='adversarial_analysis'):
    """ç»¼åˆä¿å­˜æ‰€æœ‰æ ¼å¼"""
    
    # åˆ›å»ºä¸»ç›®å½•
    if not os.path.exists(base_dir):
        os.makedirs(base_dir)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # 1. ä¿å­˜CSV
    csv_path = os.path.join(base_dir, f'adversarial_samples_{timestamp}.csv')
    save_adversarial_results(results, csv_path)
    
    # 3. ä¿å­˜å¯è¯»æ–‡æœ¬
    txt_path = os.path.join(base_dir, f'adversarial_report_{timestamp}.txt')
    save_as_readable_text(results, txt_path)
    
    # 4. åˆ†ç±»ä¿å­˜
    organize_adversarial_samples(results, os.path.join(base_dir, 'categorized_samples'))
    
    # 5. ä¿å­˜è®­ç»ƒæ•°æ®
    training_path = os.path.join(base_dir, f'adversarial_training_data_{timestamp}.csv')
    save_for_retraining(results, training_path)
    
    print(f"\nğŸ‰ æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {base_dir}/")
    print("ğŸ“‹ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  ğŸ“„ CSVæ•°æ®: adversarial_samples_{timestamp}.csv")
    print(f"  ğŸ“ æ–‡æœ¬æŠ¥å‘Š: adversarial_report_{timestamp}.txt")
    print(f"  ğŸ“ åˆ†ç±»æ ·æœ¬: categorized_samples/")
    print(f"  ğŸ¯ è®­ç»ƒæ•°æ®: adversarial_training_data_{timestamp}.csv")


if __name__ == "__main__":
    # åŠ è½½ä¸€äº›æµ‹è¯•ç”¨çš„åƒåœ¾é‚®ä»¶
    test_spam_emails = load_emails('data/english/spam')
    for email in test_spam_emails:
        email = comprehensive_preprocess(email)

    model = joblib.load('spam_model.joblib') 
    vectorizer = joblib.load('vectorizer.joblib')
    
    attacker = AdvancedAdversarialAttacker(model, vectorizer)
    results = attacker.test_attack_effectiveness(test_spam_emails)

    print("\n=== æœ€æˆåŠŸçš„å¯¹æŠ—æ ·æœ¬ ===")
    successful_attacks = [r for r in results if r['success']]
    for i, attack in enumerate(successful_attacks[:3]):
        print(f"\næ¡ˆä¾‹ {i+1}:")
        print(f"åŸå§‹: {attack['original_text']}")
        print(f"æ”»å‡»å: {attack['attacked_text']}")
        print(f"åƒåœ¾é‚®ä»¶æ¦‚ç‡: {attack['original_prob'][1]:.3f} â†’ {attack['attacked_prob'][1]:.3f}")
    comprehensive_save(results)